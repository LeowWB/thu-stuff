{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3494205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "# We recommend you to use anaconda3 to config the virtual python environment\n",
    "# !conda install pytorch torchvision torchaudio -c pytorch\n",
    "# !pip install transformers==4.24.0 scipy==1.5.0 datasets==2.7.0 promptsource==0.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe75bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thisi\\Documents\\GitHub\\bdi-hw1\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from promptsource.templates import DatasetTemplates\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from pdb import set_trace as st\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5a3b9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 438/438 [00:00<00:00, 223kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16.0k/16.0k [00:00<00:00, 108kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:01<00:00, 609kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:01<00:00, 289kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 77.0/77.0 [00:00<00:00, 73.5kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 963/963 [00:00<00:00, 1.01MB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 6.25k/6.25k [00:00<00:00, 3.05MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 36.5k/36.5k [00:00<00:00, 161kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 1.42G/1.42G [07:10<00:00, 3.30MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model BAAI/glm-roberta-large loaded.\n"
     ]
    }
   ],
   "source": [
    "# Download the model and the tokenizer\n",
    "# DEFAULT: glm-2b, which is able for good zero-shot short blanking infilling ([MASK]) and long left-to-right generation ([gMASK])\n",
    "# If you want to do fine-tuning on language understanding or generation,\n",
    "# try smaller glm-roberta-large (335M, not for zero-shot)\n",
    "model_type = 'BAAI/glm-roberta-large'#\"BAAI/glm-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type, trust_remote_code=True, revision='main')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_type, trust_remote_code=True, revision='main').half().float().cuda()\n",
    "print(f\"Model {model_type} loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0abd4d-22c9-41ae-bc42-44c7b7161a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'glm_roberta_large_original.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39faa686-5f5c-499b-9905-06a827364184",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fbf6c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Single sample:\", cond_log_prob_single_sample(model, \"One plus one equals two, is it correct? Answer: [MASK]\", [\"No\", \"Yes\"]))\\nprint(\"Batch samples:\", cond_log_prob(model, [\"Tsinghua University is located in [MASK] .\",\\n                                       \"One minus one equals zero, is it correct? Answer: [MASK]\"],\\n                                      [[\"Beijing\", \"Shanghai\"],\\n                                       [\"No\", \"Yes\"]]))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the classification task, in a Seq2Seq model like GLM, we need to calculate the conditional probability of choices for the given context.\n",
    "# Remember to refer to code example (https://github.com/THUDM/GLM#classification) in GLM's repo.\n",
    "\n",
    "# The `cond_log_prob` could be used for both multiple-choice problem (i.e., classification) or text generation (i.e., summurization).\n",
    "def cond_log_prob_single_sample(model, context, choices):\n",
    "    \"\"\"\n",
    "    Compute conditonal probability for one or more continuation/infilling options, single-sample only.\n",
    "    General solution to all classification/multiple-choice tasks.\n",
    "    :param context: prompted inputs. For example, \"One plus one equals two, is it correct? Answer: [MASK]\"\n",
    "    :param choices: classification labels or choices. For example, [\"No\", \"Yes\"]\n",
    "    \"\"\"\n",
    "    context_id = tokenizer(context)['input_ids']\n",
    "    probs = []\n",
    "    for choice in choices:\n",
    "        choice_id = tokenizer(' ' + choice)['input_ids'][1:-1]  # Feature of SentencePiece tokenizer\n",
    "        input_ids = torch.tensor(context_id + [tokenizer.sop_token_id] + choice_id[:-1], dtype=torch.long)\n",
    "        attention_mask = torch.tril(torch.ones(len(input_ids), len(input_ids), dtype=torch.long))\n",
    "        attention_mask[:len(context_id), :len(context_id)] = 1\n",
    "        mask_position = context_id.index(tokenizer.mask_token_id)\n",
    "        position_id = torch.cat([torch.arange(len(context_id)), torch.ones(len(choice_id)) * mask_position])\n",
    "        block_position_id = torch.cat([torch.zeros(len(context_id)), torch.arange(1, 1 + len(choice_id))])\n",
    "        position_id = torch.stack((position_id, block_position_id), dim=0).long()\n",
    "        logits = model.forward(input_ids=input_ids.view(1, -1).cuda(),\n",
    "                            attention_mask=attention_mask.unsqueeze(0).unsqueeze(0).cuda(),\n",
    "                            position_ids=position_id.view(1, 2, -1).cuda())['logits']\n",
    "        logits = F.log_softmax(logits, dim=-1)\n",
    "        probs.append(logits[0, range(len(context_id), len(context_id) + len(choice_id)), choice_id].sum())\n",
    "    return torch.stack(probs)\n",
    "\n",
    "# Forward results by single sample is slow. The following codes organize a batch of inputs to speed up training.\n",
    "def build_multiple_choice_sample(context, choices):\n",
    "    context_id = tokenizer(context)['input_ids']\n",
    "\n",
    "    division = len(context_id)\n",
    "    mask_position = context_id.index(tokenizer.mask_token_id)\n",
    "\n",
    "    token = np.array(context_id, dtype=np.int64)\n",
    "    attention_mask = [np.ones((division, division), dtype=np.int64)]\n",
    "    position_id = np.arange(division, dtype=np.int64)\n",
    "    block_position_id = np.zeros(division, dtype=np.int64)\n",
    "\n",
    "    choice_target_id = []\n",
    "    choice_id = []\n",
    "\n",
    "    for choice_str in choices:\n",
    "        choice = np.array(tokenizer(choice_str)['input_ids'][1:-1], dtype=np.int64)\n",
    "\n",
    "        choice_id.append(choice)\n",
    "        choice_target_id.append(np.arange(len(token), len(token) + len(choice), dtype=np.int64))\n",
    "        attention_mask.append(np.tril(np.ones((len(choice), len(choice)), dtype=np.int64)))\n",
    "\n",
    "        token = np.concatenate((token, [tokenizer.sop_token_id], choice[:-1]))\n",
    "        position_id = np.concatenate((position_id, [mask_position] * len(choice)))\n",
    "        block_position_id = np.concatenate((block_position_id, np.arange(1, 1 + len(choice), dtype=np.int64)))\n",
    "\n",
    "    attention_mask = block_diag(*attention_mask)\n",
    "    attention_mask[division:, :division] = 1\n",
    "\n",
    "    return {\n",
    "        \"token\": token,\n",
    "        \"position_id\": np.stack((position_id, block_position_id)),\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"choices\": choice_id,\n",
    "        \"choice_target_ids\": choice_target_id\n",
    "    }\n",
    "\n",
    "\n",
    "def pad_batch(tokens, position_ids, attention_mask, max_seq_length):\n",
    "    pad_length = max_seq_length - len(tokens)\n",
    "    attention_mask = np.pad(\n",
    "        attention_mask,\n",
    "        pad_width=((0, pad_length),),\n",
    "        mode=\"constant\",\n",
    "        constant_values=0,\n",
    "    )\n",
    "    tokens = np.concatenate((tokens, np.zeros(pad_length, dtype=np.int64)))\n",
    "    position_ids = np.concatenate((position_ids, position_ids[..., -1:].repeat(pad_length, -1)), axis=-1)\n",
    "    return tokens, position_ids, attention_mask\n",
    "\n",
    "\n",
    "def collate_fn(samples):\n",
    "    TILE = 16\n",
    "    length_to_pad = (max(map(lambda spl: len(spl[\"token\"]), samples)) + TILE - 1) // TILE * TILE\n",
    "\n",
    "    token_batch, position_id_batch, attention_mask_batch = [], [], []\n",
    "    choices_batch, choice_target_ids_batch = [], []\n",
    "\n",
    "    for sample in samples:\n",
    "        token, position_id, attention_mask = pad_batch(\n",
    "            sample[\"token\"], sample[\"position_id\"], sample[\"attention_mask\"], length_to_pad\n",
    "        )\n",
    "        token_batch.append(token)\n",
    "        position_id_batch.append(position_id)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "        choices_batch.append(sample[\"choices\"])\n",
    "        choice_target_ids_batch.append(sample[\"choice_target_ids\"])\n",
    "\n",
    "    return {\n",
    "        \"tokens\": torch.tensor(np.array(token_batch), dtype=torch.int64),\n",
    "        \"position_ids\": torch.tensor(np.array(position_id_batch), dtype=torch.int64),\n",
    "        \"attention_mask\": torch.tensor(np.array(attention_mask_batch), dtype=torch.int64),\n",
    "        \"choices\": choices_batch,\n",
    "        \"choice_target_ids\": choice_target_ids_batch,\n",
    "    }\n",
    "\n",
    "def cond_log_prob(model, context: List[str], choices: List[List[str]]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Compute conditonal probability for one or more continuation/infilling options.\n",
    "    :return The log probablity of each option.\n",
    "    \"\"\"\n",
    "    if not isinstance(context, list):\n",
    "        context = [context]\n",
    "        choices = [choices]\n",
    "    choices = [[(' ' + choice) for choice in choice_pair] for choice_pair in choices]  # Feature of SentencePiece tokenizer\n",
    "\n",
    "    samples = [build_multiple_choice_sample(ctx, ch) for ctx, ch in zip(context, choices)]\n",
    "\n",
    "    batch = collate_fn(samples)\n",
    "\n",
    "    logits = model.forward(input_ids=batch['tokens'].cuda(),\n",
    "                        attention_mask=batch['attention_mask'].cuda().unsqueeze(1),\n",
    "                        position_ids=batch['position_ids'].cuda())['logits']\n",
    "\n",
    "    log_probs = []\n",
    "\n",
    "    for output, choices, choice_target_ids in zip(F.log_softmax(logits, dim=-1), batch['choices'], batch['choice_target_ids']):\n",
    "        log_probs_single = []\n",
    "        for choice, choice_target_id in zip(choices, choice_target_ids):\n",
    "            tmp = output[choice_target_id, choice]\n",
    "            log_probs_single.append(tmp.sum())\n",
    "        log_probs.append(torch.stack(log_probs_single))\n",
    "\n",
    "    return torch.stack(log_probs)\n",
    "\n",
    "# print(\"Single sample:\", cond_log_prob_single_sample(model, \"One plus one equals two, is it correct? Answer: [MASK]\", [\"No\", \"Yes\"]))\n",
    "# print(\"Batch samples:\", cond_log_prob(model, [\"Tsinghua University is located in [MASK] .\",\n",
    "#                                        \"One minus one equals zero, is it correct? Answer: [MASK]\"],\n",
    "#                                       [[\"Beijing\", \"Shanghai\"],\n",
    "#                                        [\"No\", \"Yes\"]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3689eb5f-8b12-4103-a264-434018a2011e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Example: evaluating glm\\'s zero-shot perfomance on glue/sst2 using prompt from promptsource\\ndataset = load_dataset(\"hans\", split=\"validation\")\\nglue_sst2_prompts = DatasetTemplates(\\'hans\\')  # WARNING: glue/super_glue/twitter_eval datasets are not allowed in your submission. This is only an example implementation.\\nprint(\"Prompt names:\", [prompt.get_name() for prompt in glue_sst2_prompts.templates.values()])\\n# Remember to choose those prompts annotated as `original_task: true`; they are standard prompts.\\nprompt = glue_sst2_prompts[\"GPT-3 style\"]\\nchoices = prompt.answer_choices.split(\\' ||| \\')\\nprint(\"Choices:\", choices)\\n\\ncorrect = 0\\nprint(len(dataset))\\nfor i, sample in enumerate(dataset):\\n    if i % 1000 == 0:\\n        print(i, datetime.now())\\n    result = prompt.apply(sample)\\n    context = result[0] + \"Answer: [MASK]\"\\n    probs = cond_log_prob(model, context, choices)\\n    pred = torch.argmax(probs).item()\\n    correct += pred == sample[\\'label\\']\\n\\nprint(correct, correct / len(dataset))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Example: evaluating glm's zero-shot perfomance on glue/sst2 using prompt from promptsource\n",
    "# dataset = load_dataset(\"hans\", split=\"validation\")\n",
    "# glue_sst2_prompts = DatasetTemplates('hans')  # WARNING: glue/super_glue/twitter_eval datasets are not allowed in your submission. This is only an example implementation.\n",
    "# print(\"Prompt names:\", [prompt.get_name() for prompt in glue_sst2_prompts.templates.values()])\n",
    "# # Remember to choose those prompts annotated as `original_task: true`; they are standard prompts.\n",
    "# prompt = glue_sst2_prompts[\"GPT-3 style\"]\n",
    "# choices = prompt.answer_choices.split(' ||| ')\n",
    "# print(\"Choices:\", choices)\n",
    "\n",
    "# correct = 0\n",
    "# print(len(dataset))\n",
    "# for i, sample in enumerate(dataset):\n",
    "#     if i % 1000 == 0:\n",
    "#         print(i, datetime.now())\n",
    "#     result = prompt.apply(sample)\n",
    "#     context = result[0] + \"Answer: [MASK]\"\n",
    "#     probs = cond_log_prob(model, context, choices)\n",
    "#     pred = torch.argmax(probs).item()\n",
    "#     correct += pred == sample['label']\n",
    "\n",
    "# print(correct, correct / len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb7dc40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultipleChoiceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_name, split, prompt_name, tokenizer):\n",
    "        super(MultipleChoiceDataset, self).__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split = split\n",
    "        self.prompt = DatasetTemplates(self.dataset_name)[prompt_name]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Ensure that the dataset split is valid.\n",
    "        self.data = []\n",
    "        if '/' in dataset_name:\n",
    "            iters = load_dataset(dataset_name.split('/')[0], dataset_name.split('/')[1], split=split)\n",
    "        else:\n",
    "            iters = load_dataset(dataset_name, split=split)\n",
    "        for sample in tqdm(iters):\n",
    "            self.data.append(dict(zip(\n",
    "                ['inputs_pretokenized', 'choices_pretokenized', 'label'],\n",
    "                self.prompting_single_sample(sample)\n",
    "            )))\n",
    "\n",
    "    def get_choices(self, sample):\n",
    "        \"\"\"\n",
    "        Default solution for text classification.\n",
    "        TODO: not applicable to multiple-choice problem. Please customize choices from `sample`.\n",
    "        \"\"\"\n",
    "        return self.prompt.answer_choices.split(' ||| ')\n",
    "\n",
    "    def prompting_single_sample(self, sample):\n",
    "        \"\"\"\n",
    "        Format a sample into a prompted sample.\n",
    "        :return inputs_pretokenized, choices_pretokenized\n",
    "        \"\"\"\n",
    "        inputs_pretokenized, groundtruth_choice = tuple(self.prompt.apply(sample))\n",
    "        choices_pretokenized = self.get_choices(sample)\n",
    "\n",
    "        # TODO: Use default label. Please customize according to your dataset.\n",
    "        label = sample['label']\n",
    "        return inputs_pretokenized + ' [MASK]', choices_pretokenized, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca8e6bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to set model.float() and model.train() before fine-tuning, since fp16 training is instable without deepspeed.\n",
    "\n",
    "def init_logger():\n",
    "    logger = logging.getLogger(\"default\")\n",
    "    cmd_handler = logging.StreamHandler(sys.stdout)\n",
    "    cmd_handler.setLevel(logging.DEBUG)\n",
    "    cmd_handler.setFormatter(logging.Formatter(r\"[%(asctime)s][%(levelname)s][%(filename)s:%(lineno)s] %(message)s\"))\n",
    "    logger.addHandler(cmd_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def flatten_labels(compacted_labels):\n",
    "    batch_size = len(compacted_labels[0])\n",
    "    num_of_classes = len(compacted_labels)\n",
    "    return [[compacted_labels[i][idx] for i in range(num_of_classes)] for idx in range(batch_size)]\n",
    "\n",
    "\n",
    "class MultipleChoiceTrainer:\n",
    "    def __init__(self, model, epochs, lr, train_bsz, dataset_name: str, prompt_name: str):\n",
    "        self.train_bsz = train_bsz\n",
    "        self.eval_bsz = 8\n",
    "        self.epoch = epochs\n",
    "        self.lr = lr\n",
    "        # Load tokenizer & logger\n",
    "        self.tokenizer = tokenizer  # use tokenizer from 3rd cell\n",
    "        self.logger = init_logger()\n",
    "\n",
    "        # Load dataset\n",
    "        print(f'loading data')\n",
    "        self.train_dataset = MultipleChoiceDataset(dataset_name, 'train', prompt_name, self.tokenizer)\n",
    "        self.valid_dataset = MultipleChoiceDataset(dataset_name, 'validation', prompt_name, self.tokenizer)\n",
    "        print(f'train dataset length: {len(self.train_dataset)}, val dataset length: {len(self.valid_dataset)}')\n",
    "        #self.test_dataset = MultipleChoiceDataset(dataset_name, 'test', prompt_name, self.tokenizer)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.train_bsz, shuffle=True, drop_last=True)\n",
    "        self.valid_loader = DataLoader(self.valid_dataset, batch_size=self.eval_bsz, shuffle=False)\n",
    "        #self.test_loader = DataLoader(self.test_dataset, batch_size=self.eval_bsz, shuffle=False)\n",
    "\n",
    "        # Configure training model, optimizer, and scheduler\n",
    "        self.model = model  # use model from 3rd cell\n",
    "        # there should be a model.float() here. but i shifted it up (my code calls model.float() immediately after initialization)\n",
    "        self.model.train()\n",
    "        num_training_steps = self.epoch * (len(self.train_dataset) // self.train_bsz)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(self.optimizer,\n",
    "                                                         num_warmup_steps=int(num_training_steps * 0.06),\n",
    "                                                         num_training_steps=num_training_steps)\n",
    "\n",
    "    def evaluate(self, e, data_loader):\n",
    "        valid_loss = 0.0\n",
    "        valid_labels = []\n",
    "        valid_preds = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, sample in tqdm(enumerate(data_loader, start=1), desc=\"valid\", total=len(data_loader)):\n",
    "                logits = cond_log_prob(self.model, sample[\"inputs_pretokenized\"], flatten_labels(sample['choices_pretokenized']))\n",
    "                labels = sample[\"label\"].cuda()\n",
    "                loss = F.nll_loss(logits, labels)\n",
    "                valid_loss += loss.item()\n",
    "                valid_preds.extend(torch.argmax(logits, dim=-1).cpu().numpy().tolist())\n",
    "                valid_labels.extend(np.array(sample[\"label\"]).tolist())\n",
    "        valid_loss = valid_loss / len(data_loader)\n",
    "        valid_acc = accuracy_score(valid_preds, valid_labels)\n",
    "        self.logger.info(f\"[VALID] epoch {e}: loss={valid_loss}, acc={valid_acc}\")\n",
    "\n",
    "    def train(self):\n",
    "        for e in range(1, self.epoch + 1):\n",
    "            self.logger.info(f\"Epoch {e}\")\n",
    "            # train\n",
    "            tqdm_vars = {\"lr\": np.nan, \"loss\": np.nan}\n",
    "            tbar = tqdm(enumerate(self.train_loader, start=1), desc=\"train\", total=len(self.train_loader),\n",
    "                        postfix=tqdm_vars)\n",
    "            train_loss_value = 0.0\n",
    "            self.model.train()\n",
    "            print('entering training loop')\n",
    "            for i, sample in tbar:\n",
    "                logits = cond_log_prob(self.model, sample[\"inputs_pretokenized\"], flatten_labels(sample['choices_pretokenized']))\n",
    "                labels = sample[\"label\"].cuda()\n",
    "                loss = F.nll_loss(logits, labels)\n",
    "                train_loss_value += loss.item()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                tqdm_vars[\"lr\"] = self.optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "                tqdm_vars[\"loss\"] = train_loss_value\n",
    "                tbar.set_postfix(tqdm_vars)\n",
    "                train_loss_value = 0.0\n",
    "            # valid\n",
    "            self.evaluate(e, self.valid_loader)\n",
    "        # TODO: If there is a test dataset, please select the best-performed checkpoints on valid dataset to evaluate.\n",
    "        # TODO: the example `glue/rte` has no public test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6c4458-4010-4afd-ae2e-f89ad58fa525",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hans (C:/Users/thisi/.cache/huggingface/datasets/hans/plain_text/1.0.0/452e93cf5383f5ae39088254215b517d0da98ccaaf0af8f7ab04d8f23f67dbd9)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30000/30000 [01:18<00:00, 382.66it/s]\n",
      "Found cached dataset hans (C:/Users/thisi/.cache/huggingface/datasets/hans/plain_text/1.0.0/452e93cf5383f5ae39088254215b517d0da98ccaaf0af8f7ab04d8f23f67dbd9)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30000/30000 [01:18<00:00, 380.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length: 30000, val dataset length: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = MultipleChoiceTrainer(\n",
    "    model=model,\n",
    "    epochs=10,\n",
    "    lr=1e-5,\n",
    "    train_bsz=8,\n",
    "    dataset_name=\"hans\",\n",
    "    prompt_name=\"GPT-3 style\")  # Choose a `original_task: true` prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a2a7d4-3019-4b58-9b8f-f95d891db793",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3750/3750 [20:58<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-02 01:50:32,302][INFO][2370397064.py:68] [VALID] epoch 0: loss=5.620091435877482, acc=0.4997666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# trainer.evaluate(0, trainer.valid_loader)\n",
    "# output: [2023-01-02 01:50:32,302][INFO][2370397064.py:68] [VALID] epoch 0: loss=5.620091435877482, acc=0.4997666666666667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f66bd2-1902-48d6-ab31-6944cc1a8235",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-02 01:50:32,380][INFO][2370397064.py:72] Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|                                                                                                     | 0/3750 [00:00<?, ?it/s, loss=nan, lr=nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering training loop\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|                                                                                                     | 0/3750 [00:01<?, ?it/s, loss=nan, lr=nan]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.71 GiB already allocated; 0 bytes free; 1.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [8], line 82\u001b[0m, in \u001b[0;36mMultipleChoiceTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m tbar:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m---> 82\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mcond_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs_pretokenized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchoices_pretokenized\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     labels \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     84\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(logits, labels)\n",
      "Cell \u001b[1;32mIn [5], line 120\u001b[0m, in \u001b[0;36mcond_log_prob\u001b[1;34m(model, context, choices)\u001b[0m\n\u001b[0;32m    116\u001b[0m samples \u001b[38;5;241m=\u001b[39m [build_multiple_choice_sample(ctx, ch) \u001b[38;5;28;01mfor\u001b[39;00m ctx, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(context, choices)]\n\u001b[0;32m    118\u001b[0m batch \u001b[38;5;241m=\u001b[39m collate_fn(samples)\n\u001b[1;32m--> 120\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposition_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    124\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output, choices, choice_target_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(F\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoice_target_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\BAAI\\glm-roberta-large\\281b1cf6d25b9de79753dbaee57120b7cc507b64\\modeling_glm.py:893\u001b[0m, in \u001b[0;36mGLMForConditionalGeneration.forward\u001b[1;34m(self, input_ids, position_ids, attention_mask, labels, mems, **kwargs)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    885\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    886\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    892\u001b[0m ):\n\u001b[1;32m--> 893\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglm\u001b[38;5;241m.\u001b[39mforward(input_ids, position_ids, attention_mask, mems\u001b[38;5;241m=\u001b[39mmems, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    894\u001b[0m     lm_logits \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m    895\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\BAAI\\glm-roberta-large\\281b1cf6d25b9de79753dbaee57120b7cc507b64\\modeling_glm.py:782\u001b[0m, in \u001b[0;36mGLMModel.forward\u001b[1;34m(self, input_ids, position_ids, attention_mask, mems, **kwargs)\u001b[0m\n\u001b[0;32m    780\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size)\n\u001b[0;32m    781\u001b[0m \u001b[38;5;66;03m# Transformer.\u001b[39;00m\n\u001b[1;32m--> 782\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m logits, hidden_layers \u001b[38;5;241m=\u001b[39m transformer_output\n\u001b[0;32m    784\u001b[0m \u001b[38;5;66;03m# outputs = hidden_layers\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\bdi-hw1\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\BAAI\\glm-roberta-large\\281b1cf6d25b9de79753dbaee57120b7cc507b64\\modeling_glm.py:594\u001b[0m, in \u001b[0;36mGLMStack.forward\u001b[1;34m(self, hidden_states, position_ids, attention_mask, memory_states)\u001b[0m\n\u001b[0;32m    588\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    589\u001b[0m             create_custom_forward(layer),\n\u001b[0;32m    590\u001b[0m             hidden_states,\n\u001b[0;32m    591\u001b[0m             mem\u001b[38;5;241m=\u001b[39mmem_i,\n\u001b[0;32m    592\u001b[0m         )\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmem_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m     mem_layers\u001b[38;5;241m.\u001b[39mappend(check_detach(hidden_states))\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# Final layer norm.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\bdi-hw1\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\BAAI\\glm-roberta-large\\281b1cf6d25b9de79753dbaee57120b7cc507b64\\modeling_glm.py:425\u001b[0m, in \u001b[0;36mGLMBlock.forward\u001b[1;34m(self, hidden_states, ltor_mask, mem)\u001b[0m\n\u001b[0;32m    423\u001b[0m layernorm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(layernorm_input)\n\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# MLP.\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m mlp_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayernorm_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# Second residual connection.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m output \u001b[38;5;241m=\u001b[39m layernorm_input \u001b[38;5;241m+\u001b[39m mlp_output\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\bdi-hw1\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\BAAI\\glm-roberta-large\\281b1cf6d25b9de79753dbaee57120b7cc507b64\\modeling_glm.py:141\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# [b, s, 4hp]\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m     intermediate_parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_h_to_4h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     intermediate_parallel \u001b[38;5;241m=\u001b[39m gelu(intermediate_parallel)\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# [b, s, h]\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\bdi-hw1\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\bdi-hw1\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.71 GiB already allocated; 0 bytes free; 1.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a83fed-114c-4699-a6cd-a67f6a1e5914",
   "metadata": {},
   "source": [
    "u can del model and tokenizer later if u need.\n",
    "btw maybe btr to just ask the ta for gpu now or ask yvon for help maybe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c466ad-7101-4774-b121-8f1f0d950659",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44f487dd-771e-4238-b306-17c599960c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the generation task, we need to do conditional generation\n",
    "# Remember to refer to code example (https://github.com/THUDM/GLM#generation) in GLM's repo to find code for loss implementation!!!\n",
    "def generate_text(model, text, max_length=512):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=max_length)\n",
    "    inputs = {key: value.cuda() for key, value in inputs.items()}\n",
    "    # greedy decode strategy (topk = 1)\n",
    "    outputs = model.generate(**inputs, max_length=max_length, eos_token_id=tokenizer.eop_token_id, top_k=1)[0].tolist()\n",
    "    sop_id = tokenizer.sop_token_id\n",
    "    eop_id = tokenizer.eop_token_id\n",
    "    end_idx = outputs.index(eop_id) if eop_id in outputs else len(outputs)\n",
    "    return tokenizer.decode(outputs[outputs.index(sop_id) + 1: end_idx]).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d585e98-78a6-4e45-b9cd-f216acb89349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50266 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\" \"'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, ['i used to [MASK]', 'he used to [MASK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "205cc28a-3913-481d-b18e-5a7445d00b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50266 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will give the word\n"
     ]
    }
   ],
   "source": [
    "#print(generate_text(model, \"Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.\"))\n",
    "#print(generate_text(model, 'I used to rule the world. Seas would rise when I gave the word. Now, in the morning, [MASK]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c88d96d-166c-4e5a-a4ab-80781591abc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset adversarial_qa (C:/Users/thisi/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt names: ['generate_question', 'tell_what_it_is', 'question_context_answer', 'based_on', 'answer_the_following_q']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"adversarial_qa\", 'adversarialQA', split=\"validation\")\n",
    "glue_sst2_prompts = DatasetTemplates('adversarial_qa/adversarialQA')\n",
    "print(\"Prompt names:\", [prompt.get_name() for prompt in glue_sst2_prompts.templates.values()])\n",
    "prompt = glue_sst2_prompts[\"answer_the_following_q\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61476cdc-6abc-4d44-8e52-e92eff628d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following passage\n",
      "\n",
      "\"Another green space in Newcastle is the Town Moor, lying immediately north of the city centre. It is larger than London's famous Hyde Park and Hampstead Heath put together and the freemen of the city have the right to graze cattle on it. The right incidentally extends to the pitch of St. James' Park, Newcastle United Football Club's ground, though this is not exercised, although the Freemen do collect rent for the loss of privilege. Honorary freemen include Bob Geldof, King Harald V of Norway, Bobby Robson, Alan Shearer, the late Nelson Mandela and the Royal Shakespeare Company. The Hoppings funfair, said to be the largest travelling funfair in Europe, is held here annually in June.\",\n",
      "\n",
      "answer the following question. Note that the answer is present within the text.\n",
      "\n",
      "Question: Where is the Hoppings funfair held? Answer: [MASK]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50266 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Newcastle, England.\\n\\nSource: Wikipedia'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample = dataset[0]\n",
    "# result = prompt.apply(sample)\n",
    "# context = result[0] + \" Answer: [MASK]\"\n",
    "# print(context)\n",
    "# generated_text = generate_text(model, context).strip()\n",
    "# generated_text\n",
    "\n",
    "# # refer to hw3-tutrorial.pdf for guide on how to do loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2adeba7-a5e0-4ae5-bf8d-a1ac93137273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def calculate_generation_loss(model, texts, targets, max_length=512):\n",
    "#     inputs = tokenizer(texts, return_tensors=\"pt\")\n",
    "#     inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=max_length)\n",
    "#     inputs = {key: value.cuda() for key, value in inputs.items()}\n",
    "#     # greedy decode strategy (topk = 1)\n",
    "#     outputs = model.generate(**inputs, max_length=max_length, eos_token_id=tokenizer.eop_token_id, top_k=1)[0].tolist()\n",
    "#     sop_id = tokenizer.sop_token_id\n",
    "#     eop_id = tokenizer.eop_token_id\n",
    "#     end_idx = outputs.index(eop_id) if eop_id in outputs else len(outputs)\n",
    "#     return tokenizer.decode(outputs[outputs.index(sop_id) + 1: end_idx]).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95663634-77cb-42e8-a103-820a78b05c7b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (192) must match the size of tensor b (703) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(texts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbuild_inputs_for_generation(inputs, max_gen_length\u001b[38;5;241m=\u001b[39mmax_length)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     11\u001b[0m outputs\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\bdi-hw1\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\BAAI\\glm-roberta-large\\281b1cf6d25b9de79753dbaee57120b7cc507b64\\modeling_glm.py:893\u001b[0m, in \u001b[0;36mGLMForConditionalGeneration.forward\u001b[1;34m(self, input_ids, position_ids, attention_mask, labels, mems, **kwargs)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    885\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    886\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    892\u001b[0m ):\n\u001b[1;32m--> 893\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglm\u001b[38;5;241m.\u001b[39mforward(input_ids, position_ids, attention_mask, mems\u001b[38;5;241m=\u001b[39mmems, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    894\u001b[0m     lm_logits \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m    895\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\BAAI\\glm-roberta-large\\281b1cf6d25b9de79753dbaee57120b7cc507b64\\modeling_glm.py:782\u001b[0m, in \u001b[0;36mGLMModel.forward\u001b[1;34m(self, input_ids, position_ids, attention_mask, mems, **kwargs)\u001b[0m\n\u001b[0;32m    780\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size)\n\u001b[0;32m    781\u001b[0m \u001b[38;5;66;03m# Transformer.\u001b[39;00m\n\u001b[1;32m--> 782\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m logits, hidden_layers \u001b[38;5;241m=\u001b[39m transformer_output\n\u001b[0;32m    784\u001b[0m \u001b[38;5;66;03m# outputs = hidden_layers\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\bdi-hw1\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\BAAI\\glm-roberta-large\\281b1cf6d25b9de79753dbaee57120b7cc507b64\\modeling_glm.py:563\u001b[0m, in \u001b[0;36mGLMStack.forward\u001b[1;34m(self, hidden_states, position_ids, attention_mask, memory_states)\u001b[0m\n\u001b[0;32m    560\u001b[0m     position_ids, block_position_ids \u001b[38;5;241m=\u001b[39m position_ids[:, \u001b[38;5;241m0\u001b[39m], position_ids[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    561\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m--> 563\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_position_encoding:\n\u001b[0;32m    565\u001b[0m     block_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_position_embeddings(block_position_ids)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (192) must match the size of tensor b (703) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# # currently tryna compute loss. if this dun work try using the other method in the tutorial pdf file.\n",
    "# # top 3 lines r ur func args for calculate_generation_loss. just pass those into the fnc later.\n",
    "# texts = [context]\n",
    "# targets = [sample['answers']['text'][0]]\n",
    "# max_length=512\n",
    "\n",
    "# # now tryna make this thing work so we can calc loss. then shld be easy to train alr.\n",
    "# # u have the targets var above; this needs to be passed into the build_inputs_for_generation. then u can get the loss later.\n",
    "# inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "# inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=max_length).to('cuda')\n",
    "# outputs = model(**inputs)\n",
    "# outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c0de9d-9fe9-4dc1-b2c0-f655664844f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50266 for open-end generation.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cond_log_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(clp, torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# from pytorch docs: This criterion computes the cross entropy loss between input logits and target.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m----> 8\u001b[0m \u001b[43mcalculate_generation_loss2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mI used to rule the [MASK]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mworld\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [13], line 5\u001b[0m, in \u001b[0;36mcalculate_generation_loss2\u001b[1;34m(model, text, target, max_length)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_generation_loss2\u001b[39m(model, text, target, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m):\n\u001b[0;32m      4\u001b[0m     gt \u001b[38;5;241m=\u001b[39m generate_text(model, text)\n\u001b[1;32m----> 5\u001b[0m     clp \u001b[38;5;241m=\u001b[39m \u001b[43mcond_log_prob\u001b[49m(model, [text], [[target, gt]])\n\u001b[0;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(clp, torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# from pytorch docs: This criterion computes the cross entropy loss between input logits and target.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cond_log_prob' is not defined"
     ]
    }
   ],
   "source": [
    "# parallel attempt to calc loss. for now might only work for one input at a time.\n",
    "# have no idea if this'll work. but hope it does.\n",
    "def calculate_generation_loss2(model, text, target, max_length=512):\n",
    "    gt = generate_text(model, text)\n",
    "    clp = cond_log_prob(model, [text], [[target, gt]])\n",
    "    loss = F.cross_entropy(clp, torch.Tensor([0]).long().to('cuda')) # from pytorch docs: This criterion computes the cross entropy loss between input logits and target.\n",
    "    return loss\n",
    "calculate_generation_loss2(model, 'I used to rule the [MASK]', 'world') # rmb to pass in the prompt first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4081cd8-cf7a-4e5e-946a-5da40f814f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validate(model, dataset, prompt):\n",
    "    correct = 0\n",
    "    print(len(dataset))\n",
    "    for i, sample in enumerate(dataset):\n",
    "        if i % 1000 == 0:\n",
    "            print(i, datetime.now())\n",
    "        result = prompt.apply(sample)\n",
    "        context = result[0].strip() + \" Answer: [MASK]\"\n",
    "        generated_text = generate_text(model, context)\n",
    "        correct += generated_text == sample['answers']['text'][0]\n",
    "\n",
    "    print(correct, correct / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50f025c4-2251-4986-b480-e28959197caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "0 2023-01-03 00:08:33.190440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50266 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50266 for open-end generation.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_validate(model, dataset, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f4a34-664c-48e7-9f0d-2211d1c82863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConditionalGenerationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    TODO: implement your generation task dataset.\n",
    "    \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
